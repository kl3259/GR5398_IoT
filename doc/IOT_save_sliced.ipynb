{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKpJy94mHFoB",
        "outputId": "257037f9-e35c-4907-c8c8-1d729378cc54"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "import sliced\n",
        "import random\n",
        "import sklearn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sliced import SlicedAverageVarianceEstimation\n",
        "from xgboost import XGBClassifier\n",
        "# from numba import njit\n",
        "from numpy.linalg import pinv\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-k5Br3BH_Rrd"
      },
      "outputs": [],
      "source": [
        "def mahalanobis_transformation(feature_df, flag = \"Train\", feature_mean = np.empty((1,1)), sigma_pinv_sqrt = np.empty((1,1))):\n",
        "    # whiten the data\n",
        "    n = feature_df.shape[0]\n",
        "    p = feature_df.shape[1]\n",
        "    ones = np.ones((n, 1)) # n * 1\n",
        "    if flag == \"Train\":\n",
        "        # aviod np.mean in numba cannot use globals()\n",
        "        feature_mean = np.empty((1,p))\n",
        "        for i in range(p):\n",
        "            feature_mean[0,i] = np.mean(feature_df[:,i], axis = 0)\n",
        "        feature_centered = feature_df - np.dot(ones, feature_mean)\n",
        "        # get cov matrix # p * p\n",
        "        cov = np.dot(feature_centered.T, feature_centered) / n\n",
        "        # u, s, vh = np.linalg.svd(cov) # all real numbers\n",
        "        s, u = np.linalg.eigh(cov) # all real numbers\n",
        "        # theshold for pinv: 1e-15\n",
        "        s_pinv = np.linalg.pinv(np.diag(s))\n",
        "        s_pinv_sqrt = np.zeros(p)\n",
        "        for i, item in enumerate(np.diag(s_pinv)):\n",
        "            if item > 0:\n",
        "                s_pinv_sqrt[i] = np.sqrt(item)\n",
        "        sigma_pinv_sqrt = np.dot(np.dot(u, np.diag(s_pinv_sqrt)), u.T)\n",
        "        feature_df_whitened = np.dot(feature_centered, sigma_pinv_sqrt)\n",
        "    else:\n",
        "        if feature_mean is None or sigma_pinv_sqrt is None:\n",
        "            raise ValueError(\"Need to assign feature mean vector and Sigma!\")\n",
        "        if feature_mean is not None and sigma_pinv_sqrt is not None:\n",
        "            feature_centered = feature_df - np.dot(ones, feature_mean)\n",
        "            feature_df_whitened = np.dot(feature_centered, sigma_pinv_sqrt)\n",
        "    return feature_df_whitened, feature_mean, sigma_pinv_sqrt\n",
        "\n",
        "def get_cov_mat(feature_df):\n",
        "    n = feature_df.shape[0]\n",
        "    p = feature_df.shape[1]\n",
        "    ones = np.ones((n, 1))\n",
        "    feature_mean = np.empty((1,p))\n",
        "    for i in range(p):\n",
        "        feature_mean[0,i] = np.mean(feature_df[:,i])\n",
        "    feature_centered = feature_df - np.dot(ones, feature_mean)\n",
        "    # get cov matrix # p * p\n",
        "    cov = np.dot(feature_centered.T, feature_centered) / n\n",
        "    return cov\n",
        "\n",
        "def my_SAVE(feature_df_whitened, label_df, flag = \"Train\", n_directions = 977, verbose = False, **kwargs):\n",
        "\n",
        "    if flag == \"Train\":\n",
        "        # get slices\n",
        "        label_df.index = np.arange(0, label_df.shape[0], dtype = int)\n",
        "        slice_index_dict = {1: [], 2: [], 3: [], 4: [], 5: []}\n",
        "        n = feature_df_whitened.shape[0]\n",
        "        p = feature_df_whitened.shape[1]\n",
        "        for i in np.arange(5):\n",
        "            slice_index_dict[i + 1].extend(label_df[label_df[\"label\"] == (i + 1)].index)\n",
        "            globals()['feature_df_' + str(i + 1)] = feature_df_whitened[slice_index_dict[i + 1],:]\n",
        "            globals()[\"label_df_\" + str(i + 1)] = label_df.loc[slice_index_dict[i + 1], \"label\"]\n",
        "            # get sliced cov matrices\n",
        "            globals()['cov_' + str(i + 1)] = get_cov_mat(globals()[\"feature_df_\" + str(i + 1)])\n",
        "        # concatenate 5 cov matrices, weighted by the proportion of classes\n",
        "        globals()['weight_df'] = label_df[\"label\"].value_counts() / label_df.shape[0]\n",
        "        weighted_cov = pd.DataFrame(\n",
        "            data = np.zeros((globals()['cov_1'].shape[0], globals()['cov_1'].shape[0])), \n",
        "            columns = np.arange(p, dtype = int)\n",
        "        )\n",
        "        for i in np.arange(5):\n",
        "            if p == 2:\n",
        "                if verbose:\n",
        "                    u, s, vh = np.linalg.svd(globals()['cov_' + str(i + 1)])\n",
        "                    # u, s = np.linalg.eigh(globals()['cov_' + str(i + 1)])\n",
        "                    print(f'Eigen values of COV mat: {s}')\n",
        "                    print(f'Eigen values ratio of COV mat: {s[0] / s[1]:8.4f}')\n",
        "                    print(f'(1 - Eigen values of COV mat) ratio: {(1 - s[0]) / (1 - s[1]):8.4f}')\n",
        "                    print(f'(1 - Eigen values of COV mat)^2 ratio: {((1 - s[0]) / (1 - s[1])) ** 2:8.4f}')\n",
        "                    u, s, vh = np.linalg.svd(np.eye(cov_1.shape[0]) - globals()[\"cov_\" + str(i + 1)])\n",
        "                    # s, u = np.linalg.eigh(np.eye(cov_1.shape[0]) - globals()[\"cov_\" + str(i + 1)])\n",
        "                    print(f'Eigen values of (I - COV mat): {s}')\n",
        "                    print(f'Eigen values ratio of (I - COV mat) : {s[0] / s[1]:8.4f}')\n",
        "                    print(f'(1 - Eigen values of (I - COV mat)) ratio: {(1 - s[0]) / (1 - s[1]):8.4f}')\n",
        "                    print(f'(1 - Eigen values of (I - COV mat))^2 ratio: {((1 - s[0]) / (1 - s[1])) ** 2:8.4f}')\n",
        "\n",
        "            if verbose:\n",
        "                u, s, vh = np.linalg.svd(np.eye(cov_1.shape[0]) - globals()[\"cov_\" + str(i + 1)])\n",
        "                # s, u = np.linalg.eigh(np.eye(cov_1.shape[0]) - globals()[\"cov_\" + str(i + 1)])\n",
        "                print(f'(I - V) eigenvalues: {s}')\n",
        "                u, s, vh = np.linalg.svd((np.eye(cov_1.shape[0]) - globals()[\"cov_\" + str(i + 1)]) @ (np.eye(cov_1.shape[0]) - globals()[\"cov_\" + str(i + 1)]).T)\n",
        "                # s, u = np.linalg.eigh((np.eye(cov_1.shape[0]) - globals()[\"cov_\" + str(i + 1)]) @ (np.eye(cov_1.shape[0]) - globals()[\"cov_\" + str(i + 1)]).T)\n",
        "                print(f'(I - V)^2 eigenvalues: {s}')\n",
        "            temp = np.eye(cov_1.shape[0]) - globals()[\"cov_\" + str(i + 1)]\n",
        "            weighted_cov += weight_df.loc[i + 1] * temp @ temp.T\n",
        "        # u, s, vh = np.linalg.svd(weighted_cov)\n",
        "        s, u = np.linalg.eigh(weighted_cov)\n",
        "        b = u[:,:n_directions]\n",
        "        # transformed to vectors in th original scale\n",
        "        globals()['directions'] = sigma_pinv_sqrt @ b\n",
        "        feature_df_reduced = feature_df_whitened @ globals()['directions']\n",
        "        print()\n",
        "        print(f'Shape of feature df after post processing: {feature_df_reduced.shape}')  \n",
        "    elif flag == \"Test\":\n",
        "        feature_df_reduced = feature_df_whitened @ globals()['directions']\n",
        "    return feature_df_reduced, feature_df_whitened, feature_mean, sigma_pinv_sqrt\n",
        "\n",
        "def sliced_SAVE(feature_df_whitened, label_df, flag = \"Train\", n_directions = 977, **kwargs):\n",
        "    if flag == \"Train\":\n",
        "        # print('-------------------------------------------------------------------')\n",
        "        # print(f'Shape of feature df before post processing: {feature_df.shape}')\n",
        "        globals()['save'] = SlicedAverageVarianceEstimation(n_directions = n_directions, n_slices = 5)\n",
        "        globals()['save'].fit(feature_df_whitened, label_df)\n",
        "        globals()['directions'] = globals()['save'].directions_\n",
        "        feature_df_reduced = globals()['save'].transform(feature_df_whitened)\n",
        "        # print('-------------------------------------------------------------------')\n",
        "        # print(f'Shape of feature df after post processing: {feature_df_reduced.shape}') \n",
        "    else:\n",
        "        feature_df_reduced = globals()['save'].transform(feature_df_whitened)\n",
        "    return feature_df_reduced, feature_df_whitened, feature_mean, sigma_pinv_sqrt\n",
        "\n",
        "def my_lr(feature_df_train, feature_df_test, label_df_train, label_df_test):\n",
        "    lr_params_grid = {\n",
        "        'C': [0, 0.1, 0.5, 1, 10], \n",
        "        'tol': [1e-4], \n",
        "        'penalty': ['l2'],  \n",
        "        'random_state': [42], \n",
        "        'max_iter': [10000], \n",
        "        'verbose': [0], \n",
        "        'solver': ['newton-cg'], \n",
        "        'n_jobs': [6]\n",
        "    }\n",
        "\n",
        "    gs_lr = GridSearchCV(\n",
        "        estimator = LogisticRegression(), \n",
        "        param_grid = lr_params_grid, \n",
        "        cv = 5, \n",
        "        scoring = 'accuracy', \n",
        "        verbose = 0, \n",
        "        n_jobs = -1\n",
        "    )\n",
        "    gs_lr.fit(feature_df_train, np.array(label_df_train).ravel())\n",
        "    best_lr = gs_lr.best_estimator_\n",
        "    lr_pred = best_lr.predict(feature_df_test)\n",
        "    lr_f1 = f1_score(y_true = label_df_test, y_pred = lr_pred, average = 'macro')\n",
        "    lr_accu = accuracy_score(y_true = label_df_test, y_pred = lr_pred)\n",
        "    print('\\nBest params: ')\n",
        "    print(gs_lr.best_params_)\n",
        "    print(f'Accuracy: {lr_accu:8.4f}\\nF1-score: {lr_f1:8.4f}')\n",
        "    print('-------------------------------------------------------------------')\n",
        "    return lr_f1, lr_accu\n",
        "\n",
        "def my_XGB(feature_df_train, feature_df_test, label_df_train, label_df_test):\n",
        "    xgb_params_grid = {\n",
        "        'tree_method': ['gpu_hist'], \n",
        "        'n_estimators': [50, 100, 200], \n",
        "        'max_depth': [3, 4, 5, 6], \n",
        "        'n_jobs': [-1], \n",
        "        'random_state': [42], \n",
        "        'gpu_id': [0], \n",
        "        'predictor': [\"gpu_predictor\"]\n",
        "    }\n",
        "    gs = GridSearchCV(\n",
        "        estimator = XGBClassifier(), \n",
        "        param_grid = xgb_params_grid, \n",
        "        cv = 5, \n",
        "        scoring = 'accuracy', \n",
        "        verbose = 0, \n",
        "        n_jobs = -1\n",
        "    )\n",
        "    gs.fit(feature_df_train, np.array(label_df_train).ravel())\n",
        "    best_clf = gs.best_estimator_\n",
        "    pred = best_clf.predict(feature_df_test)\n",
        "    f1 = f1_score(y_true = label_df_test, y_pred = pred, average = 'macro')\n",
        "    accu = accuracy_score(y_true = label_df_test, y_pred = pred)\n",
        "    print('\\nBest params: ')\n",
        "    print(gs.best_params_)\n",
        "    print(f'Accuracy: {accu:8.4f}\\nF1-score: {f1:8.4f}')\n",
        "    print('----------------------------------------------------------------------------------------------------')\n",
        "    return f1, accu\n",
        "\n",
        "\n",
        "def generate_synthetic_data(core_dirs = 3, redundant_dirs = 7, size = 2500, test_size = 0.2, core_distribution = np.random.multivariate_normal, **kwargs): # different dist\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import random\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    total_features = core_dirs + redundant_dirs\n",
        "    mean_vec = np.random.random(size = total_features) * 10\n",
        "    feature_df = core_distribution(size = size, mean = mean_vec, **kwargs) # specify mean + cov\n",
        "    core_coef = np.random.random(size = core_dirs)\n",
        "    redundant_coef = np.zeros((redundant_dirs))\n",
        "    coef = np.concatenate([core_coef, redundant_coef], axis = 0)\n",
        "    y_train = np.sin(0.2 * np.asarray(feature_df) @ coef) * 10 + np.random.random(size = size)   \n",
        "    y_train_discrete = pd.cut(y_train, bins = 5, labels = np.arange(5) + 1).astype(int)\n",
        "    synthetic_feature_df = feature_df\n",
        "    synthetic_label_df = pd.DataFrame(y_train_discrete, columns = [\"label\"])\n",
        "    synthetic_feature_df_train, synthetic_feature_df_test, synthetic_label_df_train, synthetic_label_df_test = train_test_split(\n",
        "        synthetic_feature_df, synthetic_label_df, \n",
        "        test_size = test_size, \n",
        "        shuffle = True, \n",
        "        random_state = 42\n",
        "    )\n",
        "    return synthetic_feature_df_train, synthetic_label_df_train, synthetic_feature_df_test, synthetic_label_df_test\n",
        "\n",
        "def train_learning_curve(feature_train, label_train, feature_test, label_test, method = my_SAVE, clf = my_lr):\n",
        "    n_directions_list = list(np.arange(1, feature_train.shape[1] + 1, dtype = int))\n",
        "    f1_list = []\n",
        "    accu_list = []\n",
        "    global feature_mean\n",
        "    global sigma_pinv_sqrt\n",
        "    for n_direction in tqdm(n_directions_list):\n",
        "        globals()['feature_df_reduced_' + str(n_direction) + \"_train\"], feature_train_whitened, feature_mean, sigma_pinv_sqrt = method(\n",
        "            feature_train, \n",
        "            label_train, \n",
        "            flag = \"Train\", \n",
        "            n_directions = n_direction\n",
        "        )\n",
        "        globals()['feature_df_reduced_' + str(n_direction) + \"_test\"], feature_test_whitened, _, _ = method(\n",
        "            feature_test, \n",
        "            label_test, \n",
        "            flag = \"Test\", \n",
        "            n_directions = n_direction, \n",
        "            feature_mean = feature_mean, \n",
        "            sigma_pinv_sqrt = sigma_pinv_sqrt\n",
        "        )\n",
        "        curr_test_f1, curr_test_accu = clf(\n",
        "            feature_df_train = globals()['feature_df_reduced_' + str(n_direction) + \"_train\"], \n",
        "            feature_df_test = globals()['feature_df_reduced_' + str(n_direction) + \"_test\"], \n",
        "            label_df_train = label_train, \n",
        "            label_df_test = label_test\n",
        "        )\n",
        "        f1_list.append(curr_test_f1)\n",
        "        accu_list.append(curr_test_accu)\n",
        "\n",
        "    # train w/ original features -> syncthetic_feature_df -> whitened\n",
        "    baseline_test_f1, baseline_test_accu = my_lr(\n",
        "        feature_df_train = feature_train, \n",
        "        feature_df_test = feature_test, \n",
        "        label_df_train = label_train, \n",
        "        label_df_test = label_test\n",
        "    )\n",
        "    return f1_list, accu_list, baseline_test_f1, baseline_test_accu, n_directions_list\n",
        "\n",
        "def train_loop_video(feature_train_std, feature_test_std, label_train, label_test, test_size = 0.2, n_iteration = 1, save = True, suffix = \"\", method = sliced_SAVE):\n",
        "    if save:\n",
        "        if suffix == \"\":\n",
        "            raise ValueError(\"No filename suffix assigned!\")\n",
        "    total_features = feature_train.shape[1]\n",
        "    columns_list = list(np.arange(1, total_features + 1, dtype = int))\n",
        "    columns_list.append(\"baseline\")\n",
        "    f1_df = pd.DataFrame(data = None, columns = columns_list)\n",
        "    accu_df = pd.DataFrame(data = None, columns = columns_list)\n",
        "    label_train = np.asarray(label_train, dtype = np.float32).ravel()\n",
        "    label_test = np.asarray(label_test, dtype = np.float32).ravel()\n",
        "    for i in tqdm(np.arange(n_iteration)):\n",
        "        f1_list, accu_list = [], []\n",
        "        global feature_mean, sigma_pinv_sqrt\n",
        "        f1_list, accu_list, baseline_f1, baseline_accu, n_directions_list = train_learning_curve(\n",
        "            feature_train_std, \n",
        "            label_train, \n",
        "            feature_test_std, \n",
        "            label_test, \n",
        "            method = method, \n",
        "            clf = my_lr\n",
        "        )\n",
        "        f1_list.append(baseline_f1)\n",
        "        accu_list.append(baseline_accu)\n",
        "        f1_df = pd.concat([f1_df, pd.DataFrame([f1_list], columns = f1_df.columns, index = [i])], axis = 0)\n",
        "        accu_df = pd.concat([accu_df, pd.DataFrame([accu_list], columns = f1_df.columns, index = [i])], axis = 0)\n",
        "\n",
        "    if save:\n",
        "            f1_df.to_csv(\"../output/f1_df_\" + suffix + \".csv\", header = True)\n",
        "            accu_df.to_csv(\"../output/accu_df_\" + suffix + \".csv\", header = True)\n",
        "\n",
        "    return f1_list, accu_list, baseline_f1, baseline_accu, n_directions_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6t46_V8_SId"
      },
      "source": [
        "### Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2_yicPFSHLyH"
      },
      "outputs": [],
      "source": [
        "feature_df = pd.read_csv(\n",
        "    \"../data/feature_df_951.csv\", \n",
        "    header = 0,\n",
        "    index_col = 0\n",
        ")\n",
        "temp_label_df = pd.read_csv(\n",
        "    \"../data/label_df_951.csv\", \n",
        "    header = 0, \n",
        "    index_col = 0\n",
        ")\n",
        "label_df = pd.Series(data = temp_label_df['0'], index = temp_label_df.index, dtype = np.float32)\n",
        "feature_df = np.array(feature_df)\n",
        "\n",
        "pca = PCA(\n",
        "    n_components = 760 # maximum cols allowed\n",
        ")\n",
        "feature_df = pca.fit_transform(feature_df)\n",
        "'''\n",
        "train_test_split -> whitening for both parts -> iterations\n",
        "'''\n",
        "feature_train, feature_test, label_train, label_test = train_test_split(\n",
        "    feature_df, label_df, \n",
        "    test_size = 0.2, \n",
        "    random_state = 42, \n",
        "    shuffle = True\n",
        ")\n",
        "feature_train_std, feature_mean, sigma_pinv_sqrt = mahalanobis_transformation(\n",
        "    feature_train, \n",
        "    flag = 'Train'\n",
        ")\n",
        "feature_test_std, _, _ = mahalanobis_transformation(\n",
        "    feature_test, \n",
        "    flag = 'Test', \n",
        "    feature_mean = feature_mean, \n",
        "    sigma_pinv_sqrt = sigma_pinv_sqrt\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "std = StandardScaler()\n",
        "feature_train_std = std.fit_transform(feature_train_std)\n",
        "feature_test = std.transform(feature_test_std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTuyHz015DDE",
        "outputId": "7f306b59-008c-46a5-98f9-acab5df41a13"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "5 fits failed out of a total of 25.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1589, in fit\n",
            "    fold_coefs_ = Parallel(\n",
            "  File \"/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/site-packages/joblib/parallel.py\", line 1056, in __call__\n",
            "    self.retrieve()\n",
            "  File \"/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/site-packages/joblib/parallel.py\", line 935, in retrieve\n",
            "    self._output.extend(job.get(timeout=self.timeout))\n",
            "  File \"/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/multiprocessing/pool.py\", line 771, in get\n",
            "    raise self._value\n",
            "  File \"/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 595, in __call__\n",
            "    return self.func(*args, **kwargs)\n",
            "  File \"/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
            "    return [func(*args, **kwargs)\n",
            "  File \"/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
            "    return [func(*args, **kwargs)\n",
            "  File \"/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 216, in __call__\n",
            "    return self.function(*args, **kwargs)\n",
            "  File \"/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 822, in _logistic_regression_path\n",
            "    args = (X, target, 1.0 / C, sample_weight)\n",
            "ZeroDivisionError: float division by zero\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.23815789 0.23947368 0.23947368 0.23947368]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Best params: \n",
            "{'C': 0.5, 'max_iter': 10000, 'n_jobs': 6, 'penalty': 'l2', 'random_state': 42, 'solver': 'newton-cg', 'tol': 0.0001, 'verbose': 0}\n",
            "Accuracy:   0.1885\n",
            "F1-score:   0.0634\n",
            "-------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "5 fits failed out of a total of 25.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1589, in fit\n",
            "    fold_coefs_ = Parallel(\n",
            "  File \"/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/site-packages/joblib/parallel.py\", line 1056, in __call__\n",
            "    self.retrieve()\n",
            "  File \"/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/site-packages/joblib/parallel.py\", line 935, in retrieve\n",
            "    self._output.extend(job.get(timeout=self.timeout))\n",
            "  File \"/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/multiprocessing/pool.py\", line 771, in get\n",
            "    raise self._value\n",
            "  File \"/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 595, in __call__\n",
            "    return self.func(*args, **kwargs)\n",
            "  File \"/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
            "    return [func(*args, **kwargs)\n",
            "  File \"/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
            "    return [func(*args, **kwargs)\n",
            "  File \"/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 216, in __call__\n",
            "    return self.function(*args, **kwargs)\n",
            "  File \"/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 822, in _logistic_regression_path\n",
            "    args = (X, target, 1.0 / C, sample_weight)\n",
            "ZeroDivisionError: float division by zero\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.25789474 0.25921053 0.26052632 0.26052632]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Best params: \n",
            "{'C': 1, 'max_iter': 10000, 'n_jobs': 6, 'penalty': 'l2', 'random_state': 42, 'solver': 'newton-cg', 'tol': 0.0001, 'verbose': 0}\n",
            "Accuracy:   0.1885\n",
            "F1-score:   0.0634\n",
            "-------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "5 fits failed out of a total of 25.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1589, in fit\n",
            "    fold_coefs_ = Parallel(\n",
            "  File \"/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/site-packages/joblib/parallel.py\", line 1056, in __call__\n",
            "    self.retrieve()\n",
            "  File \"/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/site-packages/joblib/parallel.py\", line 935, in retrieve\n",
            "    self._output.extend(job.get(timeout=self.timeout))\n",
            "  File \"/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/multiprocessing/pool.py\", line 771, in get\n",
            "    raise self._value\n",
            "  File \"/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 595, in __call__\n",
            "    return self.func(*args, **kwargs)\n",
            "  File \"/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
            "    return [func(*args, **kwargs)\n",
            "  File \"/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
            "    return [func(*args, **kwargs)\n",
            "  File \"/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 216, in __call__\n",
            "    return self.function(*args, **kwargs)\n",
            "  File \"/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 822, in _logistic_regression_path\n",
            "    args = (X, target, 1.0 / C, sample_weight)\n",
            "ZeroDivisionError: float division by zero\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/Users/kangshuoli/miniforge3/envs/env_torch/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.25526316 0.26052632 0.26052632 0.26052632]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Best params: \n",
            "{'C': 0.5, 'max_iter': 10000, 'n_jobs': 6, 'penalty': 'l2', 'random_state': 42, 'solver': 'newton-cg', 'tol': 0.0001, 'verbose': 0}\n",
            "Accuracy:   0.1885\n",
            "F1-score:   0.0634\n",
            "-------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 3/760 [00:07<29:55,  2.37s/it]\n",
            "  0%|          | 0/1 [00:07<?, ?it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/tz/1kfts8hs47xb2vd2_6cggx300000gn/T/ipykernel_11859/2706314546.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# warnings.filterwarnings(\"ignore\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_loop_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_train_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_test_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"video_sliced_ver1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msliced_SAVE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/var/folders/tz/1kfts8hs47xb2vd2_6cggx300000gn/T/ipykernel_11859/564156468.py\u001b[0m in \u001b[0;36mtrain_loop_video\u001b[0;34m(feature_train_std, feature_test_std, label_train, label_test, test_size, n_iteration, save, suffix, method)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mf1_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccu_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0mfeature_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_pinv_sqrt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         f1_list, accu_list, baseline_f1, baseline_accu, n_directions_list = train_learning_curve(\n\u001b[0m\u001b[1;32m    254\u001b[0m             \u001b[0mfeature_train_std\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mlabel_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/var/folders/tz/1kfts8hs47xb2vd2_6cggx300000gn/T/ipykernel_11859/564156468.py\u001b[0m in \u001b[0;36mtrain_learning_curve\u001b[0;34m(feature_train, label_train, feature_test, label_test, method, clf)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0msigma_pinv_sqrt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn_direction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_directions_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         globals()['feature_df_reduced_' + str(n_direction) + \"_train\"], feature_train_whitened, feature_mean, sigma_pinv_sqrt = method(\n\u001b[0m\u001b[1;32m    208\u001b[0m             \u001b[0mfeature_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mlabel_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/var/folders/tz/1kfts8hs47xb2vd2_6cggx300000gn/T/ipykernel_11859/564156468.py\u001b[0m in \u001b[0;36msliced_SAVE\u001b[0;34m(feature_df_whitened, label_df, flag, n_directions, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# print(f'Shape of feature df before post processing: {feature_df.shape}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'save'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSlicedAverageVarianceEstimation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_directions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_directions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_slices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'save'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_df_whitened\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'directions'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'save'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirections_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mfeature_df_reduced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'save'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_df_whitened\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniforge3/envs/env_torch/lib/python3.9/site-packages/sliced/save.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mV_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ_slice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_slice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_slice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mM_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mV_slice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mM\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_slice\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM_slice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM_slice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;31m# eigen-decomposition of slice matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# import warnings\n",
        "# from sklearn.exceptions import ConvergenceWarning\n",
        "# warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "train_loop_video(feature_train_std, feature_test_std, label_train, label_test, test_size = 0.2, n_iteration = 1, save = True, suffix = \"video_sliced_ver1\", method = sliced_SAVE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE1zEAIA7v6t"
      },
      "source": [
        "* Reweight the score w/ confidence\n",
        "* Transformer w/o last classification layer -> doesn't take time\n",
        "* NN dimension reduction\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('env_torch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "cf4b1ea9e44270800601995b24955b9e2e1146cee5677c8b3eb4516f39ae2322"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
